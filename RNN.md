# Recurrent Neural Networks (RNN) ğŸ§ 

A comprehensive guide to understanding Recurrent Neural Networks and their applications in sequential data processing.

## ğŸ“š What is an RNN?

A **Recurrent Neural Network (RNN)** is a type of neural network designed to handle sequential data. Unlike traditional neural networks (like feedforward networks), RNNs remember previous inputs through internal memory (called hidden states).

ğŸ‘‰ **This makes them powerful for tasks where context matters** â€” e.g., the meaning of a word in a sentence can depend on the words before it.

## ğŸ—ï¸ Basic Structure of an RNN

Here's the simple architecture:

```
Input Sequence:     xâ‚ â†’ xâ‚‚ â†’ xâ‚ƒ â†’ ... â†’ xâ‚™
                    â†“    â†“    â†“         â†“
Hidden States:      hâ‚ â†’ hâ‚‚ â†’ hâ‚ƒ â†’ ... â†’ hâ‚™
                    â†“    â†“    â†“         â†“
Outputs:            yâ‚   yâ‚‚   yâ‚ƒ        yâ‚™
```

### Step-by-Step Computation

- **Input xâ‚œ**: A word (vectorized) or a value from the sequence
- **Hidden State hâ‚œ**: Acts as memory. It's influenced by the current input and the previous hidden state
- **Output yâ‚œ**: Often used for prediction or further processing

## ğŸ”‚ Why is RNN Good for Sequences?

RNNs excel at sequential data because they:

- âœ… Take previous context into account
- âœ… Share weights across time steps (parameter-efficient)
- âœ… Support variable-length input sequences

## ğŸš€ Applications

| Domain | Task |
|--------|------|
| **NLP** | Text generation, sentiment analysis, machine translation |
| **Time-series** | Stock price prediction, weather forecasting |
| **Speech** | Voice recognition, speech synthesis |
| **Healthcare** | Patient condition prediction |

## ğŸ§¨ Problems in Vanilla RNN

Despite their usefulness, vanilla RNNs face several challenges:

1. **Vanishing Gradients**: Gradients shrink over time â†’ hard to learn long-term dependencies
2. **Exploding Gradients**: Gradients grow exponentially â†’ unstable learning
3. **Short-term Memory**: Works well with short sequences but fails for long ones

### ğŸ”§ Solution
Use **LSTM** (Long Short-Term Memory) or **GRU** (Gated Recurrent Unit) â†’ advanced RNN variants that can retain long-term memory better.

## ğŸ” Example

Let's walk through a simple example with the sentence: **"I love India"**

```
Input sequence: xâ‚ = "I", xâ‚‚ = "love", xâ‚ƒ = "India"

Initialize: hâ‚€ = 0

Step 1: hâ‚ = f(Wâ‚“ * "I" + Wâ‚• * hâ‚€)
Step 2: hâ‚‚ = f(Wâ‚“ * "love" + Wâ‚• * hâ‚)
Step 3: hâ‚ƒ = f(Wâ‚“ * "India" + Wâ‚• * hâ‚‚)
```

Where:
- `Wâ‚“` = weight matrix for input
- `Wâ‚•` = weight matrix for hidden state
- `f` = activation function (typically tanh or ReLU)

The output could be a next word prediction or sentiment score.

