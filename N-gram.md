## 1. What is an N-gram?

An **n-gram** is a **sequence of 'n' consecutive items** from a given text. These items are typically **words** or **characters**.

* **Unigram (n=1)** â†’ Single words
* **Bigram (n=2)** â†’ Pairs of words  
* **Trigram (n=3)** â†’ Triplets of words
* **4-gram (n=4)** â†’ Four-word sequences, and so on

### Example

Given the sentence: `"I love machine learning"`

| Type | n-grams |
|------|---------|
| **Unigram** | `["I", "love", "machine", "learning"]` |
| **Bigram** | `["I love", "love machine", "machine learning"]` |
| **Trigram** | `["I love machine", "love machine learning"]` |

## 2. Why are N-grams Useful?

N-grams help **capture the context** and **structure of language**, especially for tasks where **word order matters**. They provide a way to model the probability of word sequences and understand linguistic patterns.

## 3. Where Does It Come in NLP?

ğŸ“Œ N-grams are studied under **Natural Language Processing (NLP)**

**Subtopics include:**
* Text preprocessing
* Language modeling
* Text classification
* Text generation
* Vectorization (feature extraction)

## 4. Applications of N-grams

| Application | Use of N-grams |
|-------------|----------------|
| ğŸ”¡ **Autocomplete** | Predict the next word based on previous words |
| ğŸ“¬ **Spam Detection** | Analyze patterns in email text using n-gram frequency |
| ğŸ“˜ **Language Modeling** | Probability models of word sequences |
| ğŸ¤– **Chatbots** | Improve understanding of user input context |
| ğŸ“ˆ **Sentiment Analysis** | Capture phrases like "not good", "very happy", etc. |
| ğŸ” **Machine Translation** | Learn word group structures and grammar |

## 5. Code Example in Python (Using NLTK)

```python
import nltk
from nltk.util import ngrams
from nltk.tokenize import word_tokenize

sentence = "I love machine learning"
tokens = word_tokenize(sentence)

# Unigrams
unigrams = list(ngrams(tokens, 1))
print("Unigrams:", unigrams)

# Bigrams
bigrams = list(ngrams(tokens, 2))
print("Bigrams:", bigrams)

# Trigrams
trigrams = list(ngrams(tokens, 3))
print("Trigrams:", trigrams)
```

**Output:**
```bash
Unigrams: [('I',), ('love',), ('machine',), ('learning',)]
Bigrams: [('I', 'love'), ('love', 'machine'), ('machine', 'learning')]
Trigrams: [('I', 'love', 'machine'), ('love', 'machine', 'learning')]
```

## 6. Limitations of N-grams

| Limitation | Explanation |
|------------|-------------|
| âŒ **Data sparsity** | As n increases, rare combinations become frequent |
| âŒ **Fixed window size** | N-grams can't model long-term dependencies |
| âŒ **Memory and computation cost** | Higher n = more memory needed for storing sequences |
| âŒ **Context loss** | Limited to fixed-size windows, missing broader context |
| âŒ **Out-of-vocabulary words** | Struggles with unseen word combinations |
