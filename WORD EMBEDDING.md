# ğŸ“˜ Word Embedding in NLP

## ğŸ§  Overview

**Word embedding** is a technique used in **Natural Language Processing (NLP)** to represent words as **dense, numerical vectors** that capture their **meanings** and **relationships** with other words.

---

## ğŸ”‘ Key Points

- Each word is mapped to a **vector of real numbers** (typically 100 or 300 dimensions).
- **Similar words** have **similar vectors** (e.g., `"king"` and `"queen"`).
- It helps machines understand the **meaning** and **context** of words.
- These vectors are **learned from large text data** using models like:
  - **Word2Vec**
  - **GloVe**
  - **FastText**

---

## ğŸ§® Example

The classic example that shows how word embeddings capture **semantic relationships**:

- vec("king") - vec("man") + vec("woman") â‰ˆ vec("queen")


This works because the embedding captures gender and role-related meaning.
# ğŸ§  Word Embedding Analogy: `vec("king") - vec("man") + vec("woman") â‰ˆ vec("queen")`

## ğŸ§  What Does It Mean?

Each word (like `"king"`, `"man"`, `"woman"`, `"queen"`) is represented as a **vector** â€” a list of numbers.

In a **well-trained embedding space** (like Word2Vec or GloVe), these vectors capture **meanings** and **relationships**, such as:

- ğŸ‘¨â€âš–ï¸ğŸ‘©â€âš–ï¸ **Gender**: male â†” female  
- ğŸ‘‘ **Royalty**: king/queen/prince/princess

These embeddings allow **mathematical operations** on word meanings.

---

## ğŸ”¢ Step-by-Step Meaning

### 1. `vec("king") - vec("man")`
- This subtracts the concept of "man" from "king".
- It gives a vector that represents **"royalty" without gender** â€” essentially, the idea of a **ruler**.

### 2. `+ vec("woman")`
- Now, add the concept of "woman" to the neutral "ruler" vector.
- This introduces **female context** into the royal concept.

### 3. `â‰ˆ vec("queen")`
- The result is a vector that points **very close** to `"queen"` in the vector space.

---

## âœ… Meaning:

> **"If a king is a man, then a queen is a woman."**

This captures the **analogy**:  


---

## ğŸ” Word Embedding vs. Vectorization

| Feature               | Vectorization                                   | Word Embedding                                               |
|-----------------------|--------------------------------------------------|---------------------------------------------------------------|
| **Definition**        | Converting words or documents into numerical vectors | Special type of vectorization that creates dense, meaningful vectors |
| **Goal**              | Make text machine-readable                       | Capture semantic meaning and relationships between words     |
| **Examples**          | One-Hot, BoW, TF-IDF                             | Word2Vec, GloVe, FastText, BERT                              |
| **Dense or Sparse?**  | Usually sparse (mostly 0s)                       | Always dense (compact, low-dimensional)                      |
| **Learns from data?** | âŒ No â€“ just counts or positions                 | âœ… Yes â€“ trained from context or co-occurrence               |
| **Captures meaning?** | âŒ No                                            | âœ… Yes                                                       |
| **Context-aware?**    | âŒ No                                            | âœ… Yes (in models like BERT, GPT)                            |
| **Used in NLP today?**| Rarely (used in simple models)                  | âœ… Widely used in modern NLP models                          |

---

## ğŸ¤“ Simplified Understanding

- **Vectorization** = Any method to convert text into numbers (**broad umbrella**).
- **Word Embedding** = Smart, dense, and learned representation (**subset of vectorization**).

---

## ğŸ§° Common Techniques

### ğŸ“¦ Vectorization Techniques:
- One-Hot Encoding
- Bag of Words (BoW)
- TF-IDF

### ğŸ’¡ Word Embedding Techniques (subset of vectorization):
- Word2Vec
- GloVe
- FastText
- BERT / GPT Embeddings

---

> ğŸ“ Word embeddings are fundamental to modern NLP applications including chatbots, sentiment analysis, machine translation, and more.

